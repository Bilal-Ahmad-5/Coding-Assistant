{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLhvV/+XYIfVGQ5Io+mIji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranabilal09/Coding-Assistant/blob/main/Self_Corrective_Coding_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UZnN0sQQOZby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "687c51fc-52ab-48ad-9d42-cb6271b5cbe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/320.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet langchain langgraph langsmith langchain_community langchain_core langchain_huggingface langchain-groq langchain-anthropic langchain-google-genai langchain-chroma faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"Gemini_Api_Key\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"langchai_api_key\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Self-Corrective-Coding-Assistant\"\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"anthropic_api_key\")"
      ],
      "metadata": {
        "id": "nHevmPDdxyru"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as Soup\n",
        "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "\n",
        "# LCEL docs\n",
        "url = \"https://python.langchain.com/docs/concepts/lcel/\"\n",
        "loader = RecursiveUrlLoader(\n",
        "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "# Sort the list based on the URLs and get the text\n",
        "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
        "d_reversed = list(reversed(d_sorted))\n",
        "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
        "    [doc.page_content for doc in d_reversed]\n",
        ")"
      ],
      "metadata": {
        "id": "_dME8DsSyrbV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "### Anthropic\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Data model\n",
        "class code(BaseModel):\n",
        "    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n",
        "\n",
        "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
        "    imports: str = Field(description=\"Code block import statements\")\n",
        "    code: str = Field(description=\"Code block not including import statements\")\n",
        "\n",
        "# Prompt to enforce tool use\n",
        "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"<instructions> You are a coding assistant with expertise in LCEL, LangChain expression language. \\n\n",
        "    Here is the LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user  question based on the \\n\n",
        "    above provided documentation. Ensure any code you provide can be executed with all required imports and variables \\n\n",
        "    defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block. \\n\n",
        "    Invoke the code tool to structure the output correctly. </instructions> \\n Here is the user question:\"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-8b\", temperature=0)\n",
        "\n",
        "structured_llm = llm.with_structured_output(code, include_raw=True)\n",
        "\n",
        "\n",
        "# Optional: Check for errors in case tool use is flaky\n",
        "def check_gemini_output(tool_output):\n",
        "    \"\"\"Check for parse error or failure to call the tool\"\"\"\n",
        "\n",
        "    # Error with parsing\n",
        "    if tool_output[\"parsing_error\"]:\n",
        "        # Report back output and parsing errors\n",
        "        print(\"Parsing error!\")\n",
        "        raw_output = str(tool_output[\"raw\"].content)\n",
        "        error = tool_output[\"parsing_error\"]\n",
        "        raise ValueError(\n",
        "            f\"Error parsing your output! Be sure to invoke the tool. Output: {raw_output}. \\n Parse error: {error}\"\n",
        "        )\n",
        "\n",
        "    # Tool was not invoked\n",
        "    elif not tool_output[\"parsed\"]:\n",
        "        print(\"Failed to invoke tool!\")\n",
        "        raise ValueError(\n",
        "            \"You did not use the provided tool! Be sure to invoke the tool to structure the output.\"\n",
        "        )\n",
        "    return tool_output\n",
        "\n",
        "\n",
        "# Chain with output check\n",
        "code_chain_raw = (\n",
        "    code_gen_prompt | structured_llm | check_gemini_output\n",
        ")\n",
        "\n",
        "\n",
        "def insert_errors(inputs):\n",
        "    \"\"\"Insert errors for tool parsing in the messages\"\"\"\n",
        "\n",
        "    # Get errors\n",
        "    error = inputs[\"error\"]\n",
        "    messages = inputs[\"messages\"]\n",
        "    messages += [\n",
        "        (\n",
        "            \"assistant\",\n",
        "            f\"Retry. You are required to fix the parsing errors: {error} \\n\\n You must invoke the provided tool.\",\n",
        "        )\n",
        "    ]\n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"context\": inputs[\"context\"],\n",
        "    }\n",
        "\n",
        "\n",
        "# This will be run as a fallback chain\n",
        "fallback_chain = insert_errors | code_chain_raw\n",
        "N = 3  # Max re-tries\n",
        "code_gen_chain_re_try = code_chain_raw.with_fallbacks(\n",
        "    fallbacks=[fallback_chain] * N, exception_key=\"error\"\n",
        ")\n",
        "\n",
        "\n",
        "def parse_output(solution):\n",
        "    \"\"\"When we add 'include_raw=True' to structured output,\n",
        "    it will return a dict w 'raw', 'parsed', 'parsing_error'.\"\"\"\n",
        "\n",
        "    return solution[\"parsed\"]\n",
        "\n",
        "\n",
        "# Optional: With re-try to correct for failure to invoke tool\n",
        "code_gen_chain = code_gen_chain_re_try | parse_output"
      ],
      "metadata": {
        "id": "VrYLVX7z8Hc0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"how do i built simple chatbot in LCEL?\"\n",
        "code_gen_chain.invoke(\n",
        "    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slw1UWpX-hb0",
        "outputId": "078acafb-4257-4cdc-b7df-df9027db818e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "code(prefix=\"This code demonstrates a simple chatbot built using LCEL. It takes user input, greets the user, and then uses an LLM to generate a response.  Replace 'YOUR_OPENAI_API_KEY' with your actual OpenAI API key.\", imports='\\\\\"\\\\\"\\\\\"Import necessary libraries\\\\\"\\\\\"\\\\\"\\\\nfrom langchain_core.runnables import RunnableSequence, RunnableLambda\\\\nfrom langchain.llms import OpenAI\\\\n', code='\\\\\"\\\\\"\\\\\"Example of a simple chatbot using LCEL\\\\\"\\\\\"\\\\\"\\\\nfrom langchain_core.runnables import RunnableSequence, RunnableLambda\\\\nfrom langchain.llms import OpenAI\\\\n\\\\n# Replace with your OpenAI API key\\\\nOPENAI_API_KEY = \\\\\"YOUR_OPENAI_API_KEY\\\\\"\\\\n\\\\n# Initialize the LLM\\\\nllm = OpenAI(openai_api_key=OPENAI_API_KEY)\\\\n\\\\ndef greet(input):\\\\n    return \\\\\"Hello! How can I help you today?\\\\\"\\\\n\\\\ndef respond(input):\\\\n    response = llm(input)\\\\n    return response\\\\n\\\\n# Define the chatbot chain\\\\nchatbot_chain = greet | respond\\\\n\\\\n# Example usage\\\\nuser_input = \\\\\"Tell me a joke\\\\\"\\\\noutput = chatbot_chain.invoke(user_input)\\\\nprint(output)')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict , Dict\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "  \"\"\"\n",
        "  Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        error : Binary flag for control flow to indicate whether test error was tripped\n",
        "        messages : With user question, error messages, reasoning\n",
        "        generation : Code solution\n",
        "        iterations : Number of tries\n",
        "  \"\"\"\n",
        "\n",
        "  error: str\n",
        "  messages: str\n",
        "  generation: str\n",
        "  iterations: int"
      ],
      "metadata": {
        "id": "bsQDXiyH4ttz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Parameter\n",
        "\n",
        "# Max tries\n",
        "max_iterations = 3\n",
        "# Reflect\n",
        "# flag = 'reflect'\n",
        "flag = \"do not reflect\"\n",
        "\n",
        "### Nodes\n",
        "\n",
        "\n",
        "def generate(state: GraphState):\n",
        "    \"\"\"\n",
        "    Generate a code solution\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---GENERATING CODE SOLUTION---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "    error = state[\"error\"]\n",
        "\n",
        "    # We have been routed back to generation with an error\n",
        "    if error == \"yes\":\n",
        "        messages += [\n",
        "            (\n",
        "                \"user\",\n",
        "                \"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\",\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    # Solution\n",
        "    code_solution = code_gen_chain.invoke(\n",
        "        {\"context\": concatenated_content, \"messages\": messages}\n",
        "    )\n",
        "    messages += [\n",
        "        (\n",
        "            \"assistant\",\n",
        "            f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Increment\n",
        "    iterations = iterations + 1\n",
        "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
        "\n",
        "\n",
        "def code_check(state: GraphState):\n",
        "    \"\"\"\n",
        "    Check code\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, error\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECKING CODE---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    code_solution = state[\"generation\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "\n",
        "    # Get solution components\n",
        "    imports = code_solution.imports\n",
        "    code = code_solution.code\n",
        "\n",
        "    # Check imports\n",
        "    try:\n",
        "        exec(imports)\n",
        "    except Exception as e:\n",
        "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
        "        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
        "        messages += error_message\n",
        "        return {\n",
        "            \"generation\": code_solution,\n",
        "            \"messages\": messages,\n",
        "            \"iterations\": iterations,\n",
        "            \"error\": \"yes\",\n",
        "        }\n",
        "\n",
        "    # Check execution\n",
        "    try:\n",
        "        exec(imports + \"\\n\" + code)\n",
        "    except Exception as e:\n",
        "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
        "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
        "        messages += error_message\n",
        "        return {\n",
        "            \"generation\": code_solution,\n",
        "            \"messages\": messages,\n",
        "            \"iterations\": iterations,\n",
        "            \"error\": \"yes\",\n",
        "        }\n",
        "\n",
        "    # No errors\n",
        "    print(\"---NO CODE TEST FAILURES---\")\n",
        "    return {\n",
        "        \"generation\": code_solution,\n",
        "        \"messages\": messages,\n",
        "        \"iterations\": iterations,\n",
        "        \"error\": \"no\",\n",
        "    }\n",
        "\n",
        "\n",
        "def reflect(state: GraphState):\n",
        "    \"\"\"\n",
        "    Reflect on errors\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---GENERATING CODE SOLUTION---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "    code_solution = state[\"generation\"]\n",
        "\n",
        "    # Prompt reflection\n",
        "\n",
        "    # Add reflection\n",
        "    reflections = code_gen_chain.invoke(\n",
        "        {\"context\": concatenated_content, \"messages\": messages}\n",
        "    )\n",
        "    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
        "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
        "\n",
        "\n",
        "### Edges\n",
        "\n",
        "\n",
        "def decide_to_finish(state: GraphState):\n",
        "    \"\"\"\n",
        "    Determines whether to finish.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "    error = state[\"error\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "\n",
        "    if error == \"no\" or iterations == max_iterations:\n",
        "        print(\"---DECISION: FINISH---\")\n",
        "        return \"end\"\n",
        "    else:\n",
        "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
        "        if flag == \"reflect\":\n",
        "            return \"reflect\"\n",
        "        else:\n",
        "            return \"generate\""
      ],
      "metadata": {
        "id": "Kvuzm1eV7Vui"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph,END\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "workflow.add_node(\"generate\",generate)\n",
        "workflow.add_node(\"code_check\",code_check)\n",
        "workflow.add_node(\"reflect\",reflect)\n",
        "workflow.add_node(\"decide_to_finish\",decide_to_finish)\n",
        "\n",
        "workflow.set_entry_point(\"generate\")\n",
        "workflow.add_edge(\"generate\", \"code_check\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"code_check\",\n",
        "    decide_to_finish,\n",
        "    {\"relect\": \"reflect\", \"generate\": \"generate\",\"end\":END},\n",
        ")\n",
        "workflow.add_edge(\"reflect\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END )\n",
        "\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "iL4lF7bV6pI_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "question = \"generate a code for biulding a voice assistant in LCEL ?\"\n",
        "\n",
        "solution = graph.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffyTfqLz93UM",
        "outputId": "8a09af59-86c3-47c8-bc62-f184c2abfc0b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: FINISH---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "solution[\"generation\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUIr9C4C_qhZ",
        "outputId": "ddd97821-6d75-4855-cca6-e8eb526d0af8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "code(prefix=\"A simple voice assistant using LangChain's LLMChain\", imports='from langchain.llms import OpenAI\\\\nfrom langchain.chains import LLMChain', code='\\\\\"\\\\\"\\\\\"Building a simple voice assistant using LCEL and OpenAI\\\\\"\\\\\"\\\\\"\\\\nfrom langchain.llms import OpenAI\\\\nfrom langchain.chains import LLMChain\\\\n\\\\n# Replace with your OpenAI API key\\\\nOPENAI_API_KEY = \\\\\"YOUR_OPENAI_API_KEY\\\\\"\\\\n\\\\n# Define the LLM\\\\nllm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\\\\n\\\\n# Define a prompt template\\\\nPROMPT = \\\\\"Respond to the following user input: \\\\\\\\n\\\\\\\\n{input}\\\\\"\\\\n\\\\n# Create an LLMChain\\\\nvoice_assistant_chain = LLMChain(llm=llm, prompt=PROMPT)\\\\n\\\\n# Example usage\\\\nuser_input = \\\\\"What is the capital of France?\\\\\"\\\\nresponse = voice_assistant_chain.run(user_input)\\\\nprint(response)')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}