{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzdZwmNYJZAoSfe0IWGG7Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammad-BILAL-5/Coding-Assistant/blob/main/Self_Corrective_Coding_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UZnN0sQQOZby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e174958e-18f9-456c-c370-ddb2b9ab91bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.7/332.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.5/252.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet langchain langgraph langchain_cohere langchain_mistralai langsmith langchain_community langchain_core langchain_huggingface langchain-groq langchain-anthropic langchain-google-genai langchain-chroma faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"Gemini_Api_Key\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"langchai_api_key\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Self-Corrective-Coding-Assistant\"\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"anthropic_api_key\")\n",
        "os.environ[\"COHERE_API_KEY\"] = userdata.get(\"cohere_api_key\")\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get(\"mistral_key\")"
      ],
      "metadata": {
        "id": "nHevmPDdxyru"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cH3njGrlunZf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as Soup\n",
        "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "\n",
        "# LCEL docs\n",
        "url = \"https://python.langchain.com/docs/concepts/lcel/\"\n",
        "loader = RecursiveUrlLoader(\n",
        "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "# Sort the list based on the URLs and get the text\n",
        "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
        "d_reversed = list(reversed(d_sorted))\n",
        "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
        "    [doc.page_content for doc in d_reversed]\n",
        ")"
      ],
      "metadata": {
        "id": "_dME8DsSyrbV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "### Anthropic\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Data model\n",
        "class code(BaseModel):\n",
        "    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n",
        "\n",
        "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
        "    imports: str = Field(description=\"Code block import statements\")\n",
        "    code: str = Field(description=\"Code block not including import statements\")\n",
        "\n",
        "\n",
        "# Prompt to enforce tool use\n",
        "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"<instructions> You are a coding assistant with expertise in LCEL, LangChain expression language. \\n\n",
        "    Here is the LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user  question based on the \\n\n",
        "    above provided documentation. Ensure any code you provide can be executed with all required imports and variables \\n\n",
        "    defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block. \\n\n",
        "    Invoke the code tool to structure the output correctly. </instructions> \\n Here is the user question:\"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-8b\", temperature=0)\n",
        "\n",
        "structured_llm = llm.with_structured_output(code, include_raw=True)\n",
        "\n",
        "\n",
        "# Optional: Check for errors in case tool use is flaky\n",
        "def check_output(tool_output):\n",
        "    \"\"\"Check for parse error or failure to call the tool\"\"\"\n",
        "\n",
        "    # Error with parsing\n",
        "    if tool_output[\"parsing_error\"]:\n",
        "        # Report back output and parsing errors\n",
        "        print(\"Parsing error!\")\n",
        "        raw_output = str(tool_output[\"raw\"].content)\n",
        "        error = tool_output[\"parsing_error\"]\n",
        "        raise ValueError(\n",
        "            f\"Error parsing your output! Be sure to invoke the tool. Output: {raw_output}. \\n Parse error: {error}\"\n",
        "        )\n",
        "\n",
        "    # Tool was not invoked\n",
        "    elif not tool_output[\"parsed\"]:\n",
        "        print(\"Failed to invoke tool!\")\n",
        "        raise ValueError(\n",
        "            \"You did not use the provided tool! Be sure to invoke the tool to structure the output.\"\n",
        "        )\n",
        "    return tool_output\n",
        "\n",
        "\n",
        "# Chain with output check\n",
        "code_chain_raw = (\n",
        "    code_gen_prompt | structured_llm | check_output\n",
        ")\n",
        "\n",
        "\n",
        "def insert_errors(inputs):\n",
        "    \"\"\"Insert errors for tool parsing in the messages\"\"\"\n",
        "\n",
        "    # Get errors\n",
        "    error = inputs[\"error\"]\n",
        "    messages = inputs[\"messages\"]\n",
        "    messages += [\n",
        "        (\n",
        "            \"assistant\",\n",
        "            f\"Retry. You are required to fix the parsing errors: {error} \\n\\n You must invoke the provided tool.\",\n",
        "        )\n",
        "    ]\n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"context\": inputs[\"context\"],\n",
        "    }\n",
        "\n",
        "\n",
        "# This will be run as a fallback chain\n",
        "gemini_fallback_chain = insert_errors | code_chain_raw\n",
        "N = 3  # Max re-tries\n",
        "code_gen_chain_re_try = code_chain_raw.with_fallbacks(\n",
        "    fallbacks=[gemini_fallback_chain] * N, exception_key=\"error\"\n",
        ")\n",
        "\n",
        "\n",
        "def parse_output(solution):\n",
        "    \"\"\"When we add 'include_raw=True' to structured output,\n",
        "    it will return a dict w 'raw', 'parsed', 'parsing_error'.\"\"\"\n",
        "\n",
        "    return solution[\"parsed\"]\n",
        "\n",
        "\n",
        "# Optional: With re-try to correct for failure to invoke tool\n",
        "gemini_code_gen_chain = code_gen_chain_re_try | parse_output"
      ],
      "metadata": {
        "id": "VrYLVX7z8Hc0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "mistral_model = \"codestral-latest\"\n",
        "llm = ChatMistralAI(model=mistral_model, temperature=0)\n",
        "\n",
        "# LLM\n",
        "mistral_code_gen_chain = llm.with_structured_output(code, include_raw=False)\n",
        "\n",
        "# Test\n",
        "question = \"Write a function for fibonacci.\"\n",
        "messages = [(\"user\", question)]\n",
        "result = mistral_code_gen_chain.invoke(messages)\n"
      ],
      "metadata": {
        "id": "qvaazq3XpAIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6f0e69-9f33-41fe-84da-c217c655d290"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Router\n",
        "\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "# Data model\n",
        "class mistral(BaseModel):\n",
        "    \"\"\"\n",
        "    The internet. Use web_search for questions that are related to anything else than agents, prompt engineering, and adversarial attacks.\n",
        "    \"\"\"\n",
        "\n",
        "    query: str = Field(description=\"The query to use when Genrating code by Mistral.mistral can respond in HTML , CSS, JAVASCRIPT, PYTHON , C# , F# , C++ and JAVA , etc.\")\n",
        "\n",
        "\n",
        "class vectorstore(BaseModel):\n",
        "    \"\"\"\n",
        "    A vectorstore containing documents used for generating code only related to retrieval augmented generation(RAG) , Gnerative AI , Langchain, Langraph Agentic workflows . Use the vectorstore for questions on these topics.\n",
        "    \"\"\"\n",
        "\n",
        "    query: str = Field(description=\"The query to use when searching the vectorstore.It includes knowledge only about LANGCHAIN, RAG and AI AGENTS \")\n",
        "\n",
        "\n",
        "# Preamble\n",
        "preamble = \"\"\"You are an expert at routing a user question to a vectorstore Code Generator or Mistral LLM Code Gnerator .\n",
        "The vectorstore contains documents used for generating code only related to retrieval augmented generation(RAG) , Langchain , Langraph Agentic workflows .\n",
        "Use the vectorstore for questions on these topics. Otherwise, use Mistral AI for generating code to do any other tasks for programers. Mistral AI Code generator can genrate code in HTML , CSS , python , javascript , java , C++ and many pther languages \"\"\"\n",
        "\n",
        "# LLM with tool use and preamble\n",
        "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
        "structured_llm_router = llm.bind_tools(\n",
        "    tools=[mistral, vectorstore], preamble=preamble\n",
        ")\n",
        "\n",
        "# Prompt\n",
        "route_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_router = route_prompt | structured_llm_router\n",
        "response = question_router.invoke(\n",
        "    {\"question\": \"how to build agentic rag?\"}\n",
        ")\n",
        "print(response.response_metadata[\"tool_calls\"])\n",
        "response = question_router.invoke({\"question\": \"build a calculator using python?\"})\n",
        "print(response.response_metadata[\"tool_calls\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hdt1L2JjC-p",
        "outputId": "7ef1dd3f-b616-44e5-d51c-251af8801015"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 'vectorstore_786wr5kdre1q', 'type': 'function', 'function': {'name': 'vectorstore', 'arguments': '{\"query\":\"agentic rag\"}'}}]\n",
            "[{'id': 'mistral_09fr79n8npr7', 'type': 'function', 'function': {'name': 'mistral', 'arguments': '{\"query\":\"python calculator code\"}'}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#routing\n",
        "\n",
        "def route_query(state):\n",
        "\n",
        "\n",
        "    print(\"---ROUTE QUESTION---\")\n",
        "    question = state[\"messages\"][-1]\n",
        "\n",
        "    source = question_router.invoke({\"question\": question})\n",
        "\n",
        "    if len(source.additional_kwargs[\"tool_calls\"]) == 0:\n",
        "        raise \"Router could not decide source\"\n",
        "\n",
        "    # Choose datasource\n",
        "    datasource = source.additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"]\n",
        "\n",
        "    if datasource == \"vectorstore\":\n",
        "        print(\"---ROUTE QUESTION TO RAG---\")\n",
        "        return {\"source\": \"vectorstore\"}\n",
        "    elif datasource == \"mistral\":\n",
        "        print(\"---ROUTE QUESTION TO MISTRAL---\")\n",
        "        return {\"source\": \"mistral\"}\n",
        "    else:\n",
        "        print(\"--- ERROR TO ROUTE QUESTION---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mQ04QhNWcDAH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict , Dict\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "  \"\"\"\n",
        "  Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        error : Binary flag for control flow to indicate whether test error was tripped\n",
        "        source : The source of the question\n",
        "        messages : With user question, error messages, reasoning\n",
        "        generation : Code solution\n",
        "        iterations : Number of tries\n",
        "  \"\"\"\n",
        "\n",
        "  error: str\n",
        "  source: str\n",
        "  messages: str\n",
        "  generation: str\n",
        "  iterations: int"
      ],
      "metadata": {
        "id": "bsQDXiyH4ttz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Parameter\n",
        "\n",
        "# Max tries\n",
        "max_iterations = 2\n",
        "# Reflect\n",
        "# flag = 'reflect'\n",
        "flag = \"do not reflect\"\n",
        "\n",
        "### Nodes\n",
        "\n",
        "\n",
        "def generate(state: GraphState):\n",
        "    \"\"\"\n",
        "    Generate a code solution\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---GENERATING CODE SOLUTION---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "    error = state[\"error\"]\n",
        "    source = state[\"source\"]\n",
        "    # We have been routed back to generation with an error\n",
        "    if error == \"yes\":\n",
        "        messages += [\n",
        "            (\n",
        "                \"user\",\n",
        "                \"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\",\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    # Solution\n",
        "    if source == \"mistral\":\n",
        "        code_solution = mistral_code_gen_chain.invoke(messages)\n",
        "    elif source == \"vectorstore\":\n",
        "        code_solution = gemini_code_gen_chain.invoke(\n",
        "            {\"context\": concatenated_content, \"messages\": messages}\n",
        "        )\n",
        "    else:\n",
        "        raise \"Router could not decide source\"\n",
        "\n",
        "    messages += [\n",
        "        (\n",
        "            \"assistant\",\n",
        "            f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Increment\n",
        "    iterations = iterations + 1\n",
        "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
        "\n",
        "\n",
        "def code_check(state: GraphState):\n",
        "    \"\"\"\n",
        "    Check code\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, error\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECKING CODE---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    code_solution = state[\"generation\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "\n",
        "    # Get solution components\n",
        "    imports = code_solution.imports\n",
        "    code = code_solution.code\n",
        "\n",
        "    # Check imports\n",
        "    try:\n",
        "        exec(imports)\n",
        "    except Exception as e:\n",
        "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
        "        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
        "        messages += error_message\n",
        "        return {\n",
        "            \"generation\": code_solution,\n",
        "            \"messages\": messages,\n",
        "            \"iterations\": iterations,\n",
        "            \"error\": \"yes\",\n",
        "        }\n",
        "\n",
        "    # Check execution\n",
        "    try:\n",
        "        exec(imports + \"\\n\" + code)\n",
        "    except Exception as e:\n",
        "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
        "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
        "        messages += error_message\n",
        "        return {\n",
        "            \"generation\": code_solution,\n",
        "            \"messages\": messages,\n",
        "            \"iterations\": iterations,\n",
        "            \"error\": \"yes\",\n",
        "        }\n",
        "\n",
        "    # No errors\n",
        "    print(\"---NO CODE TEST FAILURES---\")\n",
        "    return {\n",
        "        \"generation\": code_solution,\n",
        "        \"messages\": messages,\n",
        "        \"iterations\": iterations,\n",
        "        \"error\": \"no\",\n",
        "    }\n",
        "\n",
        "\n",
        "def reflect(state: GraphState):\n",
        "    \"\"\"\n",
        "    Reflect on errors\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation\n",
        "    \"\"\"\n",
        "    print(\"---REFLECTING---\")\n",
        "    print(\"---GENERATING CODE SOLUTION---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "    code_solution = state[\"generation\"]\n",
        "    question = state[\"messages\"][-1]\n",
        "    datasource = state[\"source\"]\n",
        "\n",
        "    if datasource == \"vectorstore\":\n",
        "        reflections = gemini_code_gen_chain.invoke(\n",
        "            {\"context\": concatenated_content, \"messages\": messages}\n",
        "        )\n",
        "    elif datasource == \"mistral\":\n",
        "        reflections = mistral_code_gen_chain.invoke(messages)\n",
        "    else:\n",
        "        raise \"Router could not decide source\"\n",
        "\n",
        "    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
        "    iterations = iterations + 1\n",
        "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
        "\n",
        "\n",
        "### Edges\n",
        "\n",
        "\n",
        "def decide_to_finish(state: GraphState):\n",
        "    \"\"\"\n",
        "    Determines whether to finish.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"---DECIDING TO FINISH---\")\n",
        "    error = state[\"error\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "    question = state[\"messages\"][-1]\n",
        "\n",
        "\n",
        "    if error == \"no\" or iterations == max_iterations:\n",
        "        print(\"---DECISION: FINISH---\")\n",
        "        return \"end\"\n",
        "    else:\n",
        "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
        "        if flag == \"reflect\":\n",
        "            return \"reflect\"\n",
        "        else:\n",
        "            return \"generate\"\n",
        ""
      ],
      "metadata": {
        "id": "Kvuzm1eV7Vui"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph,END\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "workflow.add_node(\"generate\",generate)\n",
        "workflow.add_node(\"code_check\",code_check)\n",
        "workflow.add_node(\"reflect\",reflect)\n",
        "workflow.add_node(\"decide_to_finish\",decide_to_finish)\n",
        "workflow.add_node(\"route_query\", route_query)\n",
        "\n",
        "workflow.set_entry_point(\"route_query\")\n",
        "workflow.add_edge(\"route_query\", \"generate\")\n",
        "workflow.add_edge(\"generate\", \"code_check\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"code_check\",\n",
        "    decide_to_finish,\n",
        "    {\"reflect\": \"reflect\", \"generate\": \"generate\",\"end\":END},\n",
        ")\n",
        "workflow.add_edge(\"reflect\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END )\n",
        "\n",
        "graph = workflow.compile()\n",
        ""
      ],
      "metadata": {
        "id": "iL4lF7bV6pI_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "question = \"generate code for creating a div in html?\"\n",
        "\n",
        "solution = graph.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})"
      ],
      "metadata": {
        "id": "ffyTfqLz93UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(solution[\"generation\"])"
      ],
      "metadata": {
        "id": "gUIr9C4C_qhZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}